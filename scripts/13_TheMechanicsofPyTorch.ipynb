{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scalar inputs: 1\n",
      "Scalar inputs: tensor([1])\n",
      "Scalar inputs: tensor([[1]])\n"
     ]
    }
   ],
   "source": [
    "# Creating a graph in PyTorch\n",
    "import torch\n",
    "def compute_z(a,b,c):\n",
    "    r1 = torch.sub(a,b)\n",
    "    r2 = torch.mul(r1,2)\n",
    "    z = torch.add(r2, c)\n",
    "    return z\n",
    "\n",
    "print(f\"Scalar inputs: {compute_z(torch.tensor(1), torch.tensor(2), torch.tensor(3))}\")\n",
    "print(f\"Scalar inputs: {compute_z(torch.tensor([1]), torch.tensor([2]), torch.tensor([3]))}\")\n",
    "print(f\"Scalar inputs: {compute_z(torch.tensor([[1]]), torch.tensor([[2]]), torch.tensor([[3]]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.1400, requires_grad=True)\n",
      "tensor([1., 2., 3.], requires_grad=True)\n",
      "tensor([1., 2., 3.])\n",
      "False\n",
      "tensor([1., 2., 3.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor(3.14, requires_grad=True) # I guess it corresponds to tensorflow Variable. \n",
    "print(a)\n",
    "b = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "print(b)\n",
    "\n",
    "c = torch.tensor([1.0, 2.0, 3.0], requires_grad=False)\n",
    "print(c)\n",
    "print(c.requires_grad)\n",
    "c.requires_grad_() # in-place function\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7.7071e-44, 8.1725e+20, 5.3177e-08],\n",
      "        [2.6371e-09, 2.1511e+23, 1.2794e+22]])\n",
      "tensor([[ 0.4183,  0.1688,  0.0390],\n",
      "        [ 0.3930, -0.2858, -0.1051]])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "torch.manual_seed(1)\n",
    "w = torch.empty(2,3)\n",
    "print(w)\n",
    "nn.init.xavier_normal_(w)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6018,  0.2566, -0.9591],\n",
      "        [ 0.4631,  1.4432, -0.7640]], requires_grad=True) \n",
      " tensor([[ 0.7693, -0.3316]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.w1 = torch.empty(2,3,requires_grad=True)\n",
    "        nn.init.xavier_normal_(self.w1)\n",
    "        self.w2 = torch.empty(1,2,requires_grad=True)\n",
    "        nn.init.xavier_uniform_(self.w2)\n",
    "\n",
    "mod = MyModel()\n",
    "print(mod.w1,\"\\n\",mod.w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.9000], grad_fn=<AddBackward0>)\n",
      "None None\n",
      "tensor(0.0400, grad_fn=<SumBackward0>)\n",
      "dL/dw: -0.5600, and analitical derivative: tensor([0.5600], grad_fn=<MulBackward0>)\n",
      "dL/db: -0.4000, and analitical derivative: tensor([0.4000], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Computing the gradients of the loss with respect to trainable variables\n",
    "w = torch.tensor(1.0, requires_grad=True)\n",
    "b = torch.tensor(0.5, requires_grad=True)\n",
    "\n",
    "x = torch.tensor([1.4])\n",
    "y = torch.tensor([2.1])\n",
    "\n",
    "z = torch.add(torch.mul(w, x), b)\n",
    "print(z)\n",
    "# z.backward()\n",
    "print(w.grad, b.grad)\n",
    "\n",
    "loss = (y-z).pow(2).sum()\n",
    "print(loss)\n",
    "\n",
    "loss.backward()\n",
    "print(f\"dL/dw: {w.grad:.4f}, and analitical derivative: {2 * (y - (w*x + b)) * x}\")\n",
    "print(f\"dL/db: {b.grad:.4f}, and analitical derivative: {2 * (y - (w*x + b))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear\n",
      "ReLU\n",
      "ReLU\n",
      "Linear\n",
      "ReLU\n",
      "Sequential\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.9815,  0.0155, -0.0966,  0.2447],\n",
       "        [-0.1593,  0.6428,  0.2464,  0.4166],\n",
       "        [-0.0223, -0.0953, -0.0662,  0.4552],\n",
       "        [-0.4342,  0.1690, -0.1215,  0.7060],\n",
       "        [ 0.0596, -0.0632, -0.3166,  0.2364],\n",
       "        [ 0.0452, -0.3048,  0.1651,  0.2656],\n",
       "        [ 0.5015, -0.2668,  0.0858,  0.2185],\n",
       "        [ 0.5222,  0.5635,  0.3242, -0.1645],\n",
       "        [ 0.0913,  0.2883,  0.3237, -0.0338],\n",
       "        [-0.2596, -0.0228,  0.4489, -0.1574],\n",
       "        [ 0.1405,  0.3110, -0.0946,  0.0627],\n",
       "        [ 0.4120, -0.3310, -0.2239,  0.3932],\n",
       "        [-0.1227,  0.1710,  0.2576,  0.1151],\n",
       "        [ 0.4222, -0.1967,  0.2515,  0.6993],\n",
       "        [-0.0163,  0.3057, -0.5568, -0.0113],\n",
       "        [-0.5595,  0.0929, -0.4644,  0.4916]], requires_grad=True)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def weights_init(m):\n",
    "    # print(m)\n",
    "    classname = m.__class__.__name__\n",
    "    print(classname)\n",
    "    if classname.find('Linear') != -1:\n",
    "        torch.nn.init.zeros_(m.weight)\n",
    "        # torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        torch.nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "        torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(4, 16),\n",
    "    # nn.init.xavier_normal(),\n",
    "    nn.ReLU(),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16, 32),\n",
    "    # nn.init.xavier_normal(),\n",
    "    nn.ReLU()\n",
    ")\n",
    "model.apply(weights_init)\n",
    "# model.state_dict()\n",
    "model[0].weight\n",
    "\n",
    "nn.init.xavier_normal_(model[0].weight)\n",
    "model[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing a loss function"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "364514d90753a2d691a6d767d7cc8424b11bd444a2479e7755202f2080322fb8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
